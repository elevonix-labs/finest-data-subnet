"""Data Processing and Deduplication Pipeline Script.

This script processes data from a given S3 bucket, applies several filters,
and performs deduplication using Minhash.

Usage:
    python main.py --hf_repo <HF account repo> --data_url <data URL> --total_tasks <number of tasks> --cpus_per_task <number of CPUs per task> --limit <optional limit>

Example:
    python main.py --hf_repo barney49/original_data --total_tasks 4 --cpus_per_task 32 --limit 1000
"""


from datetime import datetime
import argparse
from dotenv import load_dotenv
import os
import bittensor as bt
import nltk
import time
from miner.get_task import fetch_warc_files, send_finish_request
from miner.upload_to_hf import upload_dataset
from miner.refining_dataset import DataRefiner
import asyncio
import shutil
import logging
from utils import assert_registered
from generate import generate_signature

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.StreamHandler(),  # Outputs to the console
        logging.FileHandler('commit_processing.log', mode='w')  # Logs to a file
    ],
)

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading 'punkt' package...")
    nltk.download('punkt')
    
def get_config() -> bt.config:
    """
    Initialize and parse command-line arguments and add Bittensor-specific arguments.

    Returns:
        bt.Config: Parsed configuration.
    """
    parser = argparse.ArgumentParser(description="Upload dataset to Hugging Face and commit dataset URL to Bittensor subtensor chain.")
    # parser.add_argument("--hf_repo", type=str,  help="The Hugging Face repository to upload the dataset.")
    parser.add_argument("--netuid", type=str, default=250, help="The unique identifier for the network.")
    parser.add_argument("--hf_repo", type=str, help="Link to the repo on huggingface.")
    parser.add_argument('--total_tasks', type=int, default=4, help='Total number of tasks')
    parser.add_argument('--cpus_per_task', type=int, default=32, help='Number of CPUs per task')
    parser.add_argument('--limit', type=int, default=-1, help='Number of records to process in WarcReader')

    # Add Bittensor-specific arguments
    bt.wallet.add_args(parser)
    bt.subtensor.add_args(parser)
    bt.logging.add_args(parser)

    config = bt.config(parser)
    return config
def remove_result_folder(folder_path):
    shutil.rmtree(folder_path)
    print(f"Folder '{folder_path}' removed successfully.")

async def main(config):
    """
    Main function to commit dataset to Bittensor subtensor chain.

    Args:
        config (bt.Config): Configuration object.
    """
    print(f"bittensor version: {bt.__version__}")


    while True:  # Infinite loop to keep the script running continuously
        start = time.time()
        # Initialize logging
        bt.logging(config=config)
        # Initialize wallet and subtensor
        wallet = bt.wallet(config=config)
        subtensor = bt.subtensor(config=config)
        timestamp = datetime.now()
        timezone = timestamp.astimezone().tzname()

        message =f"{timestamp}{timezone}"
        signature = generate_signature(wallet, message)
        # Retrieve the metagraph
        metagraph: bt.metagraph = subtensor.metagraph(config.netuid)

        # Ensure the wallet is registered
        hotkey, _ = assert_registered(wallet, metagraph)
        logging.info(f"Hotkey: {hotkey} is registered ")
        warc_files = fetch_warc_files(hotkey, message, signature)
        logging.info(f"Received {len(warc_files)} warc files")

        if not warc_files:  
            bt.logging.info("WARC files not found, waiting for 2 hours before retrying...")
            await asyncio.sleep(2 * 3600)  
            continue  

        result_path = f"./result"
        # # Remove result path if it already exists
        if os.path.exists(result_path):
            logging.info(f"Removing result folder {result_path}")
            remove_result_folder(result_path)
        
        logging.info(f"Refining {len(warc_files)} warc files")
        refiner = DataRefiner(warc_files, result_path, config.total_tasks, config.cpus_per_task, config.limit)
        processing_success = refiner.refine()

        if processing_success:
            logging.info("Data processing completed successfully")
            bt.logging.success("ðŸŽ‰ Data processing completed successfully")

            hf_repo_hash = upload_dataset(result_path, config.hf_repo)
            
            if hf_repo_hash:
                while True:
                    try:
                        logging.info(f"Committing dataset to subtensor chain {hf_repo_hash}:{config.hf_repo}")
                        subtensor.commit(wallet, config.netuid, f"{hf_repo_hash}:{config.hf_repo}")
                        bt.logging.success("ðŸŽ‰ Successfully committed dataset to subtensor chain")
                        break
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                        bt.logging.error(f"Error while committing to subtensor chain: {e}, retrying in 300 seconds...")
                        await asyncio.sleep(300)
                
                max_retries = 10
                retry_count = 0

                while retry_count < max_retries:
                    try:
                        logging.info(f"Sending finish request for hotkey {hotkey}")
                        message =f"{timestamp}{timezone}"
                        signature = generate_signature(wallet, message)
                        response = send_finish_request(hotkey, message, signature, config.hf_repo)
                        if response:
                            break
                    except Exception as e:
                        logging.error(f"Error while sending finish request: {e}")
                        await asyncio.sleep(20)
                        retry_count += 1
        end = time.time() - start
        print(f"Processing time: {end:.2f} seconds")

        await asyncio.sleep(8 * 3600) 

if __name__ == "__main__":

    load_dotenv()

    config = get_config()

    logging.info("Starting the mining")

    asyncio.run(main(config))
